{"cells": [{"cell_type": "markdown", "source": ["# \ud83d\udd01 Attention-based Seq2Seq Training \u2013 Teaching Notebook\n", "This notebook trains a French-to-English translation model using Bahdanau-style attention with Keras Functional API."]}, {"cell_type": "code", "source": ["import tensorflow as tf\n", "from tensorflow.keras.models import Model\n", "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate\n", "from tensorflow.keras.preprocessing.sequence import pad_sequences\n", "from utils.tokenizer_utils import create_tokenizers, save_tokenizers\n", "\n", "# Load dataset\n", "lines = open(\"../data/french_english_pairs.txt\", encoding='utf-8').read().strip().split('\\n')\n", "input_texts, target_texts = zip(*[line.split('\\t') for line in lines])"]}, {"cell_type": "code", "source": ["# Preprocess & tokenize\n", "tokenizer_in, tokenizer_out = create_tokenizers(input_texts, target_texts)\n", "input_seq = tokenizer_in.texts_to_sequences(input_texts)\n", "target_seq = tokenizer_out.texts_to_sequences(target_texts)\n", "max_len = 10\n", "encoder_input_data = pad_sequences(input_seq, maxlen=max_len, padding='post')\n", "decoder_input_data = pad_sequences(target_seq, maxlen=max_len, padding='post')"]}, {"cell_type": "code", "source": ["# Save tokenizers\n", "save_tokenizers(tokenizer_in, tokenizer_out, \"../utils/tokenizer_in.pkl\", \"../utils/tokenizer_out.pkl\")"]}, {"cell_type": "code", "source": ["# Define attention-based Seq2Seq model\n", "latent_dim = 128\n", "vocab_in = len(tokenizer_in.word_index) + 1\n", "vocab_out = len(tokenizer_out.word_index) + 1\n", "\n", "encoder_inputs = Input(shape=(None,))\n", "enc_emb = Embedding(vocab_in, latent_dim)(encoder_inputs)\n", "encoder_outputs, state_h, state_c = LSTM(latent_dim, return_sequences=True, return_state=True)(enc_emb)\n", "\n", "decoder_inputs = Input(shape=(None,))\n", "dec_emb = Embedding(vocab_out, latent_dim)(decoder_inputs)\n", "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n", "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n", "\n", "# Attention\n", "score_dense = Dense(1, activation=\"tanh\")\n", "attention_scores = score_dense(encoder_outputs)\n", "attention_weights = tf.nn.softmax(attention_scores, axis=1)\n", "context_vector = tf.reduce_sum(attention_weights * encoder_outputs, axis=1)\n", "context_vector = tf.expand_dims(context_vector, 1)\n", "decoder_combined_context = Concatenate(axis=-1)([context_vector, decoder_outputs])\n", "\n", "decoder_dense = Dense(vocab_out, activation='softmax')\n", "decoder_outputs = decoder_dense(decoder_combined_context)\n", "\n", "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n", "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n", "model.fit([encoder_input_data, decoder_input_data], tf.expand_dims(decoder_input_data, -1), batch_size=2, epochs=30, verbose=1)\n", "model.save(\"../models/attention_seq2seq.h5\")"]}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 2}