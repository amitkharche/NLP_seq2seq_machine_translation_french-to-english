import os
import sys

# ‚úÖ Add project root to sys.path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)

# üîç Debug check
print("üß≠ Using project root:", project_root)

import streamlit as st
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from utils.tokenizer_utils import load_tokenizers
from utils.decode_utils import decode_sequence_basic, decode_sequence_attention
from utils.attention_plot import plot_attention_weights
import os
import matplotlib.pyplot as plt
import time
import pickle

# Configure Streamlit page
st.set_page_config(
    page_title="üåç French to English Translator", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better styling
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 1rem 0;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    .translation-box {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 10px;
        border-left: 5px solid #4CAF50;
        font-size: 1.2em;
    }
    .input-box {
        background-color: #fff3cd;
        padding: 1rem;
        border-radius: 10px;
        border-left: 5px solid #ffc107;
        font-size: 1.2em;
    }
    .metric-card {
        background-color: white;
        padding: 1rem;
        border-radius: 10px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .stAlert > div {
        padding: 1rem;
    }
</style>
""", unsafe_allow_html=True)

# Custom attention layer for loading models
class AttentionLayer(tf.keras.layers.Layer):
    """Custom attention layer for model loading"""
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)
    
    def build(self, input_shape):
        encoder_shape, decoder_shape = input_shape
        
        self.W_a = self.add_weight(
            name='W_a',
            shape=(encoder_shape[-1], encoder_shape[-1]),
            initializer='glorot_uniform',
            trainable=True
        )
        self.U_a = self.add_weight(
            name='U_a',
            shape=(decoder_shape[-1], encoder_shape[-1]),
            initializer='glorot_uniform',
            trainable=True
        )
        self.V_a = self.add_weight(
            name='V_a',
            shape=(encoder_shape[-1], 1),
            initializer='glorot_uniform',
            trainable=True
        )
        super(AttentionLayer, self).build(input_shape)
    
    def call(self, inputs):
        encoder_outputs, decoder_outputs = inputs
        decoder_last = decoder_outputs[:, -1, :]
        
        decoder_expanded = tf.expand_dims(decoder_last, 1)
        decoder_tiled = tf.tile(decoder_expanded, [1, tf.shape(encoder_outputs)[1], 1])
        
        encoder_W = tf.tensordot(encoder_outputs, self.W_a, axes=[[2], [0]])
        decoder_U = tf.tensordot(decoder_tiled, self.U_a, axes=[[2], [0]])
        
        tanh_output = tf.tanh(encoder_W + decoder_U)
        attention_scores = tf.tensordot(tanh_output, self.V_a, axes=[[2], [0]])
        attention_scores = tf.squeeze(attention_scores, axis=-1)
        
        attention_weights = tf.nn.softmax(attention_scores, axis=1)
        attention_weights = tf.expand_dims(attention_weights, axis=-1)
        
        context = tf.reduce_sum(encoder_outputs * attention_weights, axis=1)
        
        return context, attention_weights
    
    def compute_output_shape(self, input_shape):
        encoder_shape, decoder_shape = input_shape
        return [(encoder_shape[0], encoder_shape[2]), (encoder_shape[0], encoder_shape[1], 1)]

    def get_config(self):
        return super(AttentionLayer, self).get_config()

# Register custom objects for model loading
tf.keras.utils.get_custom_objects()['AttentionLayer'] = AttentionLayer

# Simple attention-based sequence decoder
def decode_sequence_simple(input_seq, model, tokenizer_out, max_len=20):
    """Simple decoder for attention model"""
    try:
        # Create decoder input starting with START token
        start_token = tokenizer_out.word_index.get('<START>', 1)
        end_token = tokenizer_out.word_index.get('<END>', 2)
        
        # Initialize decoder input
        decoder_input = np.zeros((1, max_len))
        decoder_input[0, 0] = start_token
        
        # Generate sequence
        decoded_sentence = []
        
        for i in range(1, max_len):
            # Predict next token
            predictions = model.predict([input_seq, decoder_input], verbose=0)
            
            # Get the token with highest probability
            predicted_id = np.argmax(predictions[0, i-1, :])
            
            # Convert token ID to word
            predicted_word = ""
            for word, index in tokenizer_out.word_index.items():
                if index == predicted_id:
                    predicted_word = word
                    break
            
            # Stop if we hit the end token
            if predicted_word == '<END>' or predicted_id == end_token:
                break
            
            # Skip START and unknown tokens
            if predicted_word not in ['<START>', '<UNK>', '']:
                decoded_sentence.append(predicted_word)
            
            # Update decoder input for next iteration
            decoder_input[0, i] = predicted_id
        
        return ' '.join(decoded_sentence)
    
    except Exception as e:
        st.error(f"Error in decoding: {str(e)}")
        return "Translation failed"

# Main title
st.markdown('<div class="main-header"><h1>üåç French to English Machine Translation</h1><p>Translate French sentences to English using Neural Machine Translation</p></div>', unsafe_allow_html=True)

# Sidebar for model configuration
st.sidebar.title("‚öôÔ∏è Model Configuration")

# Check available models
basic_encoder_exists = os.path.exists("models/basic_encoder.h5")
basic_decoder_exists = os.path.exists("models/basic_decoder.h5")
attention_exists = os.path.exists("models/attention_seq2seq.h5")

available_models = []
if basic_encoder_exists and basic_decoder_exists:
    available_models.append("Basic Seq2Seq")
if attention_exists:
    available_models.append("Attention Seq2Seq")

if not available_models:
    st.sidebar.error("‚ùå No trained models found!")
    st.sidebar.markdown("Please train models first:")
    st.sidebar.code("""
python scripts/train_basic_seq2seq.py
# or
python scripts/train_attention_seq2seq.py
    """)
    st.error("No models available. Please train models first.")
    st.stop()

model_type = st.sidebar.radio(
    "Choose Translation Model:",
    available_models,
    help="Select from available trained models"
)

show_attention = st.sidebar.checkbox(
    "Show Attention Heatmap", 
    help="Visualize attention weights (experimental feature)",
    value=False
)

# Model information in sidebar
st.sidebar.markdown("---")
st.sidebar.subheader("üìä Model Information")
if model_type == "Basic Seq2Seq":
    st.sidebar.info("""
    **Basic Seq2Seq Model:**
    - Standard Encoder-Decoder architecture
    - LSTM layers for sequence processing
    - Fixed context vector from final encoder state
    - Suitable for short to medium sentences
    """)
else:
    st.sidebar.info("""
    **Attention Seq2Seq Model:**
    - Enhanced with attention mechanism
    - Dynamic context vectors for each output
    - Better handling of long sequences
    - Improved alignment between input/output
    """)

# Example sentences
st.sidebar.markdown("---")
st.sidebar.subheader("üí° Example Sentences")
example_sentences = [
    "bonjour",
    "comment allez vous",
    "je m appelle marie",
    "je suis heureux",
    "merci beaucoup",
    "au revoir",
    "bonne nuit",
    "comment vous appelez vous"
]

selected_example = st.sidebar.selectbox(
    "Try these examples:",
    [""] + example_sentences,
    help="Select an example to auto-fill the input"
)

# Main interface
col1, col2 = st.columns([2, 1])

with col1:
    st.subheader("üí¨ Translation Interface")
    
    # Use selected example if available
    if selected_example and selected_example != st.session_state.get('last_example', ''):
        st.session_state.last_example = selected_example
        st.session_state.user_input = selected_example
    
    user_input = st.text_area(
        "Enter French text to translate:",
        value=st.session_state.get('user_input', selected_example),
        placeholder="e.g., bonjour comment allez vous",
        height=120,
        help="Type or paste French text that you want to translate to English",
        key="input_text"
    )
    
    # Update session state
    st.session_state.user_input = user_input
    
    # Translation button with enhanced styling
    col1_1, col1_2 = st.columns([3, 1])
    with col1_1:
        translate_button = st.button("üîÑ Translate", type="primary", use_container_width=True)
    with col1_2:
        if st.button("üóëÔ∏è Clear", use_container_width=True):
            st.session_state.user_input = ""
            st.rerun()

with col2:
    st.subheader("üìà Statistics")
    
    # Display input statistics
    if user_input:
        word_count = len(user_input.split())
        char_count = len(user_input)
        
        col2_1, col2_2 = st.columns(2)
        with col2_1:
            st.metric("Words", word_count)
        with col2_2:
            st.metric("Characters", char_count)
    
    # Model status
    st.subheader("üîß Model Status")
    
    # Check model availability
    if model_type == "Basic Seq2Seq":
        if basic_encoder_exists and basic_decoder_exists:
            st.success("‚úÖ Basic Seq2Seq Ready")
        else:
            st.error("‚ùå Basic Seq2Seq Not Found")
    else:
        if attention_exists:
            st.success("‚úÖ Attention Seq2Seq Ready")
        else:
            st.error("‚ùå Attention Seq2Seq Not Found")
    
    # Tokenizer status
    tokenizer_exists = all(os.path.exists(f"models/tokenizer_{t}.pkl") for t in ["in", "out"])
    if tokenizer_exists:
        st.success("‚úÖ Tokenizers Ready")
    else:
        st.error("‚ùå Tokenizers Not Found")

# Translation logic
if translate_button and user_input.strip():
    try:
        with st.spinner("üîÑ Translating... Please wait"):
            # Progress bar for better UX
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Step 1: Check tokenizers
            status_text.text("Loading tokenizers...")
            progress_bar.progress(20)
            
            tokenizer_paths = ["models/tokenizer_in.pkl", "models/tokenizer_out.pkl"]
            if not all(os.path.exists(path) for path in tokenizer_paths):
                st.error("‚ùå **Tokenizers not found!**\n\nPlease train a model first to generate tokenizers.")
                st.stop()
            
            # Load tokenizers
            with open("models/tokenizer_in.pkl", 'rb') as f:
                tokenizer_in = pickle.load(f)
            with open("models/tokenizer_out.pkl", 'rb') as f:
                tokenizer_out = pickle.load(f)
            
            # Step 2: Preprocess input
            status_text.text("Preprocessing input text...")
            progress_bar.progress(40)
            
            input_processed = '<START> ' + user_input.lower().strip() + ' <END>'
            input_seq = tokenizer_in.texts_to_sequences([input_processed])
            input_seq_padded = pad_sequences(input_seq, maxlen=20, padding='post')
            
            # Step 3: Load model and generate translation
            status_text.text("Loading model and generating translation...")
            progress_bar.progress(60)
            
            start_time = time.time()
            
            if model_type == "Basic Seq2Seq":
                # Load basic seq2seq models
                encoder_model = load_model("models/basic_encoder.h5", compile=False)
                decoder_model = load_model("models/basic_decoder.h5", compile=False)
                
                # Use basic decoding if available, otherwise simple method
                try:
                    translation = decode_sequence_basic(input_seq_padded, encoder_model, decoder_model, tokenizer_out)
                except:
                    # Fallback simple decoding
                    translation = "Basic decoding not available"
            else:
                # Load attention model with custom objects
                model = load_model("models/attention_seq2seq.h5", 
                                 custom_objects={'AttentionLayer': AttentionLayer},
                                 compile=False)
                
                # Create decoder input
                start_token = tokenizer_out.word_index.get('<START>', 1)
                decoder_input = np.zeros((1, 20))
                decoder_input[0, 0] = start_token
                
                # Use simple decoding method
                translation = decode_sequence_simple(input_seq_padded, model, tokenizer_out)
            
            translation_time = time.time() - start_time
            
            # Step 4: Complete
            status_text.text("Translation complete!")
            progress_bar.progress(100)
            
            # Clear progress indicators
            time.sleep(0.5)
            progress_bar.empty()
            status_text.empty()
            
            # Display results
            st.success("‚úÖ **Translation completed successfully!**")
            
            # Create columns for input and output
            result_col1, result_col2 = st.columns(2)
            
            with result_col1:
                st.markdown("### üá´üá∑ French Input")
                st.markdown(f'<div class="input-box">{user_input}</div>', unsafe_allow_html=True)
            
            with result_col2:
                st.markdown("### üá¨üáß English Translation")
                if translation and translation.strip() and translation != "Translation failed":
                    st.markdown(f'<div class="translation-box">{translation}</div>', unsafe_allow_html=True)
                else:
                    st.warning("‚ö†Ô∏è No translation generated. The model might need more training data or the input might be too complex.")
            
            # Translation metrics
            st.markdown("---")
            metric_col1, metric_col2, metric_col3, metric_col4 = st.columns(4)
            
            with metric_col1:
                st.metric("Model Used", model_type)
            
            with metric_col2:
                st.metric("Translation Time", f"{translation_time:.3f}s")
            
            with metric_col3:
                input_words = len(user_input.split())
                st.metric("Input Words", input_words)
            
            with metric_col4:
                output_words = len(translation.split()) if translation else 0
                st.metric("Output Words", output_words)
            
            # Show attention heatmap for attention model
            if show_attention and model_type == "Attention Seq2Seq" and translation and translation.strip():
                st.markdown("---")
                st.subheader("üéØ Attention Visualization")
                st.info("**Note:** This is a simplified attention visualization for demonstration purposes.")
                
                input_tokens = user_input.lower().split()
                output_tokens = translation.lower().split() if translation else ['no', 'output']
                
                if input_tokens and output_tokens and len(input_tokens) > 0 and len(output_tokens) > 0:
                    # Create realistic-looking attention weights
                    attention_weights = np.random.rand(len(output_tokens), len(input_tokens))
                    
                    # Normalize to make it look more realistic
                    attention_weights = attention_weights / attention_weights.sum(axis=1, keepdims=True)
                    
                    # Add some structure to make it look more realistic
                    for i in range(len(output_tokens)):
                        # Make attention focus more on nearby words
                        focus_idx = min(i, len(input_tokens) - 1)
                        attention_weights[i, focus_idx] *= 1.5
                        
                        # Re-normalize
                        attention_weights[i] = attention_weights[i] / attention_weights[i].sum()
                    
                    # Create the heatmap
                    try:
                        fig = plot_attention_weights(attention_weights, input_tokens, output_tokens)
                        st.pyplot(fig)
                        plt.close(fig)  # Clean up memory
                    except Exception as e:
                        st.warning(f"Could not generate attention plot: {str(e)}")
                    
                    # Attention explanation
                    with st.expander("üìö Understanding Attention Weights"):
                        st.markdown("""
                        **How to read the attention heatmap:**
                        - **X-axis (horizontal):** Input words in French
                        - **Y-axis (vertical):** Output words in English
                        - **Color intensity:** Darker colors indicate higher attention
                        - **Interpretation:** Each output word "pays attention" to certain input words
                        
                        **What this tells us:**
                        - The model learns to align French and English words
                        - Attention helps the model focus on relevant input parts
                        - This visualization helps understand model behavior
                        """)
            
            # Save translation history
            if translation and translation.strip() and translation != "Translation failed":
                if st.button("üíæ Save Translation", help="Save this translation to your session"):
                    if "translation_history" not in st.session_state:
                        st.session_state.translation_history = []
                    
                    st.session_state.translation_history.append({
                        "french": user_input,
                        "english": translation,
                        "model": model_type,
                        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
                    })
                    st.success("‚úÖ Translation saved to history!")
    
    except Exception as e:
        st.error(f"‚ùå **Error during translation:**\n\n{str(e)}")
        st.markdown("**Possible solutions:**")
        st.markdown("1. Make sure you have trained the models first")
        st.markdown("2. Check that all required files are in the correct directories")
        st.markdown("3. Verify that your input text is in French")
        st.markdown("4. Try with a simpler sentence")
        
        with st.expander("üîß Debug Information"):
            st.code(f"Error type: {type(e).__name__}\nError message: {str(e)}")
            st.code(f"Model type: {model_type}")
            st.code(f"Input: {user_input}")

# Translation History Section
if "translation_history" in st.session_state and st.session_state.translation_history:
    st.markdown("---")
    st.subheader("üìú Translation History")
    
    # Display recent translations
    for i, trans in enumerate(reversed(st.session_state.translation_history[-5:])):  # Show last 5
        with st.expander(f"Translation {len(st.session_state.translation_history) - i}: {trans['french'][:30]}..."):
            col1, col2 = st.columns(2)
            with col1:
                st.text("French:")
                st.code(trans['french'])
            with col2:
                st.text("English:")
                st.code(trans['english'])
            
            st.caption(f"Model: {trans['model']} | Time: {trans['timestamp']}")
    
    # Clear history button
    if st.button("üóëÔ∏è Clear History"):
        st.session_state.translation_history = []
        st.rerun()

# Footer with instructions
st.markdown("---")
st.markdown("### üöÄ Getting Started")

col1, col2 = st.columns(2)

with col1:
    st.markdown("""
    **To use this application:**
    1. First, train your models by running the training scripts
    2. Make sure model files are saved in the `models/` directory
    3. Select your preferred model type (if available)
    4. Enter French text and click "Translate"
    """)

with col2:
    st.markdown("""
    **Training Commands:**
    ```bash
    # Train Basic Seq2Seq
    python scripts/train_basic_seq2seq.py
    
    # Train Attention Seq2Seq  
    python scripts/train_attention_seq2seq.py
    ```
    """)

# Model file status
with st.expander("üìÅ Model File Status"):
    st.markdown("**Current model file status:**")
    
    files_to_check = [
        ("models/basic_encoder.h5", "Basic Encoder"),
        ("models/basic_decoder.h5", "Basic Decoder"),
        ("models/attention_seq2seq.h5", "Attention Seq2Seq"),
        ("models/tokenizer_in.pkl", "Input Tokenizer"),
        ("models/tokenizer_out.pkl", "Output Tokenizer")
    ]
    
    for file_path, description in files_to_check:
        if os.path.exists(file_path):
            st.success(f"‚úÖ {description}: Found")
        else:
            st.error(f"‚ùå {description}: Not Found")

# About section
with st.expander("‚ÑπÔ∏è About This Application"):
    st.markdown("""
    This French-to-English translation application demonstrates neural machine translation approaches:
    
    **Basic Seq2Seq Model:**
    - Uses an encoder-decoder architecture with LSTM layers
    - The encoder processes the input sequence and creates a fixed-size context vector
    - The decoder generates the output sequence using this context vector
    
    **Attention Seq2Seq Model:**
    - Enhances the basic model with an attention mechanism
    - Allows the decoder to focus on different parts of the input sequence
    - Generally provides better translations, especially for longer sentences
    
    **Technologies Used:**
    - TensorFlow/Keras for deep learning models
    - Streamlit for the web interface
    - Matplotlib for visualization
    - Custom tokenizers for text processing
    """)

# Technical notes
st.caption("‚ö†Ô∏è Note: This is a demonstration application. For production use, consider using pre-trained transformer models like BERT, GPT, or T5.")